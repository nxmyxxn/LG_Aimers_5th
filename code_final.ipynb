{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxmyxxn/LG_Aimers_5th/blob/main/code_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW1jD8Z6u_PH"
      },
      "outputs": [],
      "source": [
        "# ! pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Wxc4Bz0ku_PK",
        "outputId": "52ccbb32-f0e2-4071-e926-b212335e5a45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/elicer/data/train.csv\n",
            "/home/elicer/data/test.csv\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from typing import List, Optional\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import faiss\n",
        "from scipy.stats import mode\n",
        "import category_encoders as ce\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.metrics import f1_score\n",
        "from catboost import Pool,CatBoostClassifier\n",
        "\n",
        "\n",
        "DO_VALID = False\n",
        "\n",
        "ROOT_PATH=os.path.abspath('.')\n",
        "\n",
        "if DO_VALID:\n",
        "    TRAIN_PATH = os.path.join(ROOT_PATH, 'data', 'processed', 'train.csv')\n",
        "    VALID_PATH = os.path.join(ROOT_PATH, 'data', 'processed', 'valid.csv')\n",
        "else:\n",
        "    TRAIN_PATH = os.path.join(ROOT_PATH, 'data', 'train.csv')\n",
        "    VALID_PATH = None\n",
        "TEST_PATH = os.path.join(ROOT_PATH, 'data', 'test.csv')\n",
        "\n",
        "print(TRAIN_PATH)\n",
        "print(TEST_PATH)\n",
        "print(VALID_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATnYx0UYu_PM"
      },
      "source": [
        "# Model1: Vector Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zb20iYKu_PO"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE=110"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_05ZMWaau_PP"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(TRAIN_PATH)\n",
        "if DO_VALID:\n",
        "    valid = pd.read_csv(VALID_PATH)\n",
        "test = pd.read_csv(TEST_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAQHSRZou_PQ"
      },
      "outputs": [],
      "source": [
        "def label2id(y):\n",
        "    return pd.Series(y).map({'Normal':0, 'AbNormal':1})\n",
        "\n",
        "def id2label(y):\n",
        "    return pd.Series(y).map({0:'Normal', 1:'AbNormal'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XhwZqwZu_PR"
      },
      "source": [
        "## 1.1 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KibtLPgmu_PS"
      },
      "outputs": [],
      "source": [
        "class BasicPreprocessor:\n",
        "    def __call__(self, df,\n",
        "                 ok2nan=False,\n",
        "                 drop_unique_cols=False,\n",
        "                 drop_duplicated_cols=False):\n",
        "        # 온점(.)과 공백을 언더바(_)로 교체\n",
        "        df.columns = df.columns.str.replace('.', '_')\n",
        "        df.columns = df.columns.str.replace(' ', '_')\n",
        "\n",
        "        # 잘못된 OK 값을 np.nan으로 대체\n",
        "        if ok2nan:\n",
        "            df = self.replace_ok_to_nan(df)\n",
        "\n",
        "        # 모든 row 가 unique한 경우 Drop\n",
        "        if drop_unique_cols:\n",
        "            unique_cols = self.find_unique_columns(df)\n",
        "            df = df.drop(columns=unique_cols)\n",
        "\n",
        "        # 값이 완전히 동일한 column들 제거\n",
        "        if drop_duplicated_cols:\n",
        "            df = self.drop_duplicated_features( df, 'Model_Suffix')\n",
        "            df = self.drop_duplicated_features( df, 'Workorder')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def replace_ok_to_nan(self,df):\n",
        "        cols = [\"HEAD_NORMAL_COORDINATE_X_AXIS(Stage1)_Collect_Result_Dam\",\n",
        "                \"HEAD_NORMAL_COORDINATE_X_AXIS(Stage1)_Collect_Result_Fill1\",\n",
        "                \"HEAD_NORMAL_COORDINATE_X_AXIS(Stage1)_Collect_Result_Fill2\"]\n",
        "        for col in cols:\n",
        "            if col in df.columns:\n",
        "                df.loc[df[col] == \"OK\", col] = np.nan\n",
        "                df[col] = df[col].astype(float)\n",
        "                print('After replacement:' , df[col].unique())\n",
        "        return df\n",
        "\n",
        "\n",
        "    def find_unique_columns(self, df):\n",
        "        unique_domain_columns = []\n",
        "        for column in df.columns:\n",
        "            unique_values = df[column].dropna().unique()\n",
        "            if len(unique_values) <= 1:\n",
        "                unique_domain_columns.append(column)\n",
        "        return unique_domain_columns\n",
        "\n",
        "\n",
        "    def drop_duplicated_features(self, df, core_name:str):\n",
        "        duplicated = [feature for feature in df.columns if core_name in feature]\n",
        "        if duplicated:\n",
        "            df = df.rename(columns={duplicated.pop():core_name})\n",
        "            df = df.drop(duplicated, axis=1)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ03rc7Du_PT",
        "outputId": "e3610d59-a403-426d-cfe3-1b547d46ed51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After replacement: [162.4   nan 549.5 549.  550.3 550.  548.5]\n",
            "After replacement: [837.7   nan 838.4 837.9 838.2 837.5]\n",
            "After replacement: [305.    nan 835.5]\n",
            "After replacement: [  nan 550.3 162.4 549.  549.5 550.  548.5]\n",
            "After replacement: [  nan 838.4 837.7 837.9 838.2 837.5]\n",
            "After replacement: [  nan 835.5 305. ]\n"
          ]
        }
      ],
      "source": [
        "preprocessor = BasicPreprocessor()\n",
        "test = preprocessor(test, ok2nan=True, drop_unique_cols=True, drop_duplicated_cols=True)\n",
        "train = preprocessor(train, ok2nan = True,  drop_unique_cols=True, drop_duplicated_cols=True)\n",
        "if DO_VALID:\n",
        "    valid = preprocessor(valid, ok2nan=True, drop_duplicated_cols=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpTiTJrIu_PU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PreprocessorForVectorSimilarity(BasicPreprocessor):\n",
        "    def __init__(self, cols_to_label_enc:Optional[List[str]]=None):\n",
        "        self.mode_dict= None\n",
        "        self.workorder_agg = None\n",
        "        self.label_encoders = {}\n",
        "        self.scalers = {}\n",
        "        self.selectors = {}\n",
        "        self.pcas = {}\n",
        "        self.is_train=None\n",
        "\n",
        "        if cols_to_label_enc is not None:\n",
        "            self.cols_to_label_enc = cols_to_label_enc\n",
        "        else:\n",
        "            self.cols_to_label_enc = ['Equipment_Dam', 'Chamber_Temp__Judge_Value_AutoClave', 'Equipment_Fill1', 'Equipment_Fill2']\n",
        "\n",
        "    def __call__(self, df, is_train):\n",
        "        self.is_train=is_train\n",
        "        df = self._fill_na_with_mode(df)\n",
        "        df = self._add_workorder_stat(df)\n",
        "        df = self._label_encoding(df)\n",
        "\n",
        "        X = df.select_dtypes(exclude='object')\n",
        "        if ('target' in df.columns) or is_train:\n",
        "            y= df['target']\n",
        "        else:\n",
        "            y=None\n",
        "\n",
        "        vectors_all, y_all = self._preprocess_for_subprocess(X, y, 'All', n_components=30)\n",
        "        vectors_dam, y_dam= self._preprocess_for_subprocess(X, y, 'Dam', n_components=20)\n",
        "        vectors_fl1, y_fl1 = self._preprocess_for_subprocess(X, y, 'Fill1', n_components=20)\n",
        "        vectors_fl2, y_fl2 = self._preprocess_for_subprocess(X, y, 'Fill2', n_components=20)\n",
        "        vectors_ac,  y_ac = self._preprocess_for_subprocess(X, y, 'AutoClave', n_components=13)\n",
        "\n",
        "        return {\n",
        "            'All':(vectors_all, y_all),\n",
        "            'Dam': (vectors_dam, y_dam),\n",
        "            'Fill1':(vectors_fl1, y_fl1),\n",
        "            'Fill2':(vectors_fl2, y_fl2),\n",
        "            'AutoClave':(vectors_ac, y_ac)\n",
        "        }\n",
        "\n",
        "    def _fill_na_with_mode(self, df):\n",
        "        def check_null(df):\n",
        "            # checking missing data\n",
        "            total = df.isnull().sum().sort_values(ascending=False)\n",
        "            percent = (df.isnull().sum() / df.isnull().count()*100).sort_values(ascending=False)\n",
        "            missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "            return missing_data\n",
        "\n",
        "\n",
        "        if self.is_train:\n",
        "            mode_dict= dict()\n",
        "            # 결측치 확인\n",
        "            df_na = check_null(df)\n",
        "            has_na = df_na[df_na['Total'] != 0].index.tolist()\n",
        "\n",
        "            # 결측치 있을 경우 최빈값 대체\n",
        "            if has_na:\n",
        "                for col in has_na:\n",
        "                    mode = df[col].value_counts().sort_values(ascending=False).index[0]\n",
        "                    df[col] = df[col].fillna(value=mode)\n",
        "                    mode_dict[col]=mode\n",
        "\n",
        "            # column명 및 최빈값 저장\n",
        "            self.mode_dict = mode_dict\n",
        "        else:\n",
        "            for col, mode in self.mode_dict.items():\n",
        "                df[col] = df[col].fillna(value=mode)\n",
        "        return df\n",
        "\n",
        "    def _add_workorder_stat(self, df):\n",
        "        if self.is_train:\n",
        "            numeric = df.select_dtypes(exclude='object').columns.tolist()\n",
        "            workorder_agg= df.groupby('Workorder', as_index=False)[numeric].agg(['std','mean']).reset_index()\n",
        "\n",
        "            columns = ['index', 'Workorder']\n",
        "            for var in workorder_agg.columns.levels[0]:\n",
        "                if var not in ['index', 'Workorder']:\n",
        "                    for stat in workorder_agg.columns.levels[1][:-1]:\n",
        "                        columns.append(f'WO_{var}_{stat}')\n",
        "\n",
        "            workorder_agg.columns = columns\n",
        "            workorder_agg = workorder_agg.dropna(axis=1)\n",
        "            workorder_agg = workorder_agg.drop('index', axis=1)\n",
        "\n",
        "            self.workorder_agg = workorder_agg\n",
        "\n",
        "        df = df.merge(self.workorder_agg, on='Workorder', how='left').fillna(value=0)\n",
        "        return df\n",
        "\n",
        "    def _label_encoding(self, df):\n",
        "        for col in self.cols_to_label_enc:\n",
        "            new_col = f'{col}_encoded'\n",
        "            if self.is_train:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                label_enc = self.label_encoders[col]\n",
        "                df[new_col] = label_enc.fit_transform(df[col])\n",
        "            else:\n",
        "                label_enc = self.label_encoders[col]\n",
        "                df[new_col] = label_enc.transform(df[col])\n",
        "        return df\n",
        "\n",
        "    def _preprocess_for_subprocess(self,\n",
        "                                  X,\n",
        "                                  y,\n",
        "                                  subprocess:str,\n",
        "                                  n_components:int\n",
        "                                  ):\n",
        "\n",
        "        X = self._get_process_df(subprocess, X)\n",
        "        if self.is_train:\n",
        "            X, y = self._oversampling(X, y)\n",
        "\n",
        "            self.scalers[subprocess] = StandardScaler()\n",
        "            scaler = self.scalers[subprocess]\n",
        "\n",
        "            self.selectors[subprocess] = VarianceThreshold(threshold=0.05)\n",
        "            selector =self.selectors[subprocess]\n",
        "\n",
        "            self.pcas[subprocess] =  PCA(n_components=n_components)\n",
        "            pca = self.pcas[subprocess]\n",
        "\n",
        "            vectors = scaler.fit_transform(X)\n",
        "            vectors = selector.fit_transform(vectors)\n",
        "            vectors = pca.fit_transform(vectors)\n",
        "        else:\n",
        "            scaler = self.scalers[subprocess]\n",
        "            selector =self.selectors[subprocess]\n",
        "            pca = self.pcas[subprocess]\n",
        "\n",
        "            vectors = scaler.transform(X)\n",
        "            vectors = selector.transform(vectors)\n",
        "            vectors = pca.transform(vectors)\n",
        "\n",
        "        vectors = self._preprocess_for_vector_db(vectors)\n",
        "\n",
        "        return vectors, y\n",
        "\n",
        "    def _get_process_df(self, subprocess, X):\n",
        "        if subprocess=='All':\n",
        "            return X\n",
        "        else:\n",
        "            cols = [col for col in X.columns if subprocess in col]\n",
        "        return X.loc[:, cols]\n",
        "\n",
        "    def _oversampling(self, X, y):\n",
        "        y = y.map({'Normal':0, 'AbNormal':1})\n",
        "        smote = SMOTE(random_state=RANDOM_STATE)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "\n",
        "        print(f\"  Total: Normal: {(y == 0).sum()}, AbNormal: {(y == 1).sum()}\")\n",
        "        y = y.map({0:'Normal', 1:'AbNormal'})\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    def _preprocess_for_vector_db(self, vectors):\n",
        "        vectors = np.array(vectors, dtype=np.float32)\n",
        "        vectors = np.ascontiguousarray(vectors)\n",
        "        faiss.normalize_L2(vectors)\n",
        "        return vectors\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CIf9ytE9u_PV",
        "outputId": "78aad938-64b0-4ed9-d93c-139573d35517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Total: Normal: 38156, AbNormal: 38156\n",
            "  Total: Normal: 38156, AbNormal: 38156\n",
            "  Total: Normal: 38156, AbNormal: 38156\n",
            "  Total: Normal: 38156, AbNormal: 38156\n",
            "  Total: Normal: 38156, AbNormal: 38156\n"
          ]
        }
      ],
      "source": [
        "vector_preprocessor = PreprocessorForVectorSimilarity()\n",
        "train_dict = vector_preprocessor(train, is_train=True)\n",
        "if DO_VALID:\n",
        "    valid_dict = vector_preprocessor(valid, is_train=False)\n",
        "test_dict = vector_preprocessor(test, is_train=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UmLCLZu_PV"
      },
      "source": [
        "## 1.2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP2lbTURu_PW"
      },
      "outputs": [],
      "source": [
        "class VectorSimilaritySubModel():\n",
        "    def __init__(self, train_vectors, train_y):\n",
        "        self.train_y = train_y\n",
        "        self.index = faiss.IndexFlatIP(train_vectors.shape[1])\n",
        "        self.index.add(train_vectors)\n",
        "\n",
        "    def predict(self, vectors, k = 7, th = 6):\n",
        "        y_pred = []\n",
        "        for i, vector in enumerate(vectors):\n",
        "            # if i > 10:\n",
        "            #     break\n",
        "            vector = vector.reshape(1, -1)\n",
        "            distances, indices = self.index.search(vector, k)\n",
        "\n",
        "            labels = self.train_y.iloc[indices.flatten()].tolist()\n",
        "            cnt = labels.count('AbNormal')\n",
        "            pred = 'AbNormal' if cnt > th else 'Normal'\n",
        "            y_pred.append(pred)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "class VectorSimilarityModel():\n",
        "    def __init__(self, train_dict):\n",
        "        self.model_all = VectorSimilaritySubModel(*train_dict['All'])\n",
        "        self.model_dam = VectorSimilaritySubModel(*train_dict['Dam'])\n",
        "        self.model_fl1 = VectorSimilaritySubModel(*train_dict['Fill1'])\n",
        "        self.model_fl2 = VectorSimilaritySubModel(*train_dict['Fill2'])\n",
        "        self.model_ac = VectorSimilaritySubModel(*train_dict['AutoClave'])\n",
        "\n",
        "    def predict(self, vector_dict):\n",
        "        y_pred = self.model_all.predict(vector_dict['All'][0])\n",
        "        y_pred_dam = self.model_dam.predict(vector_dict['Dam'][0], k=5, th=4)\n",
        "        y_pred_fl1 = self.model_fl1.predict(vector_dict['Fill1'][0], k=10, th=7)\n",
        "        y_pred_fl2 = self.model_fl2.predict(vector_dict['Fill2'][0], k=15, th=7)\n",
        "        y_pred_ac = self.model_ac.predict(vector_dict['AutoClave'][0], k=15, th=11)\n",
        "\n",
        "        predictions  = [\n",
        "            label2id(y_pred),\n",
        "            label2id(y_pred_dam),\n",
        "            label2id(y_pred_fl1),\n",
        "            label2id(y_pred_fl2),\n",
        "            label2id(y_pred_ac)\n",
        "        ]\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        return mode(predictions, axis=0)[0]\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGgCNrmSu_PW"
      },
      "outputs": [],
      "source": [
        "model = VectorSimilarityModel(train_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKyNBWqFu_PW"
      },
      "source": [
        "## 1.3 Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5QMPXaku_PX"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    y_pred = model.predict(valid_dict)\n",
        "    y_pred = id2label(y_pred)\n",
        "    y_valid = valid_dict['All'][1]\n",
        "    f1 = f1_score(y_valid, y_pred, pos_label='AbNormal')\n",
        "    print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QYuOCBiu_PX"
      },
      "outputs": [],
      "source": [
        "# y_valid.to_csv('val_vector_similarity.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0z6RZM6u_PX"
      },
      "source": [
        "##  1.3 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g1gKb-mu_PX"
      },
      "outputs": [],
      "source": [
        "test_pred  = model.predict(test_dict)\n",
        "test_pred = id2label(test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uouFzsoWu_PY",
        "outputId": "dc97fece-cca8-49ef-fa80-e9e13f546e63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "Normal      16869\n",
              "AbNormal      492\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 529,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission_1.csv\", index=False)\n",
        "\n",
        "df_sub\n",
        "\n",
        "df_sub['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T3xfazFu_PY"
      },
      "source": [
        "# Model2: RF+CatBoost and Isolaiton Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inEBfRcou_PY"
      },
      "source": [
        "## 2.1 Read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXTFPoAnu_PY"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "DO_VALID = True\n",
        "TRAIN_PATH = os.path.join(ROOT_PATH, 'data', 'processed', 'train.csv')\n",
        "VALID_PATH = os.path.join(ROOT_PATH, 'data', 'processed', 'valid.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDACtHLQu_PY"
      },
      "outputs": [],
      "source": [
        "train_path = TRAIN_PATH\n",
        "if DO_VALID:\n",
        "    valid_path = VALID_PATH\n",
        "\n",
        "drop_cols = [\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\",\n",
        "            \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\",\n",
        "            \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\",\n",
        "            ]\n",
        "\n",
        "def read_csv(path, drop_cols = None):\n",
        "    data = pd.read_csv(path)\n",
        "    data = data.sort_index().reset_index(drop=True)\n",
        "    labels = data[[\"target\"]]\n",
        "\n",
        "    features = data.drop(\"target\", axis=1)\n",
        "    if drop_cols is not None:\n",
        "        features = data.drop(drop_cols, axis=1)\n",
        "    return features, labels\n",
        "\n",
        "X_train_raw, y_train_raw = read_csv(TRAIN_PATH) #, drop_cols)\n",
        "if DO_VALID:\n",
        "    X_valid_raw, y_valid_raw = read_csv(VALID_PATH) #, drop_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "dhhKYktau_PY",
        "outputId": "f301840b-6577-4194-d265-ea5418bfacdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Set:\n",
            " target  \n",
            "Normal      31156\n",
            "AbNormal     2000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Valid Set:\n",
            " target  \n",
            "Normal      7000\n",
            "AbNormal     350\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Set:\\n\", y_train_raw.value_counts())\n",
        "print()\n",
        "if DO_VALID:\n",
        "    print(\"Valid Set:\\n\", y_valid_raw.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbAnHAHRu_PZ"
      },
      "source": [
        "## 2.2 결측치 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RwdITZ22u_PZ",
        "outputId": "ac92a659-ff28-461c-e70b-64f977ad9bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59028\n"
          ]
        }
      ],
      "source": [
        "print(X_train_raw.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eih-bIKuu_PZ"
      },
      "outputs": [],
      "source": [
        "def find_unique_columns(df):\n",
        "    unique_domain_columns = []\n",
        "    for column in df.columns:\n",
        "        unique_values = df[column].dropna().unique()\n",
        "        if len(unique_values) <= 1:\n",
        "            unique_domain_columns.append(column)\n",
        "    return unique_domain_columns\n",
        "# 모든 row 가 unique한 경우 Drop\n",
        "\n",
        "unique_cols = find_unique_columns(X_train_raw)\n",
        "X_train_raw = X_train_raw.drop(columns=unique_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzwKQ_Ktu_PZ"
      },
      "outputs": [],
      "source": [
        "def fill_na_with_mean(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():\n",
        "            mean_value = df[column].mean()\n",
        "            df[column].fillna(mean_value, inplace=True)\n",
        "\n",
        "def fill_na_with_knn(df, is_test=False, imputers=None):\n",
        "    if imputers is None:\n",
        "        imputers = {}\n",
        "    base = \"HEAD NORMAL COORDINATE \"\n",
        "    stage = \" AXIS(Stage1) Collect Result_\"\n",
        "    for process in ['Dam', 'Fill1', 'Fill2']:\n",
        "        relevant_columns = []\n",
        "        for axis in ['X', 'Y', 'Z']:\n",
        "            col_name = base + axis + stage + process\n",
        "            relevant_columns.append(col_name)\n",
        "\n",
        "        target_col = base+'X'+stage+process\n",
        "        df_subset = df[relevant_columns].copy()\n",
        "\n",
        "        if is_test:\n",
        "            assert imputers is not None\n",
        "            imputer = imputers[target_col]\n",
        "            imputed_values = imputer.transform(df_subset)\n",
        "        else:\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "            imputed_values = imputer.fit_transform(df_subset)\n",
        "            imputers[target_col] = imputer\n",
        "        df[target_col] = imputed_values[:, 0]\n",
        "    return imputers\n",
        "\n",
        "# fill_na_with_mean(X_train_raw)\n",
        "# fill_na_with_mean(X_test_raw)\n",
        "\n",
        "im = fill_na_with_knn(X_train_raw)\n",
        "if DO_VALID:\n",
        "    _ = fill_na_with_knn(X_valid_raw, is_test=True, imputers=im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "eCVG_PZJu_Pa",
        "outputId": "679f2382-cd03-4d62-ea7e-40ec6f328324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(X_train_raw.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S-cO7MPu_Pa"
      },
      "source": [
        "## 2.3 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0cjvRv0u_Pa"
      },
      "outputs": [],
      "source": [
        "def cb_preprocessing(x, y=None):\n",
        "    if y is not None:\n",
        "        y = y.replace({'AbNormal': 1, 'Normal': 0})\n",
        "    categorical_columns = x.select_dtypes(exclude=[np.number]).columns\n",
        "    return x, y, categorical_columns\n",
        "\n",
        "def rf_preprocessing(x, y=None, is_test=False, TargetEncoder=None):\n",
        "    if y is not None:\n",
        "        y = y.replace({'AbNormal': 1, 'Normal': 0})\n",
        "    categorical_columns = x.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    if is_test:\n",
        "        assert TargetEncoder is not None\n",
        "        x = TargetEncoder.transform(x)\n",
        "        target_encoder = None\n",
        "    else:\n",
        "        assert y is not None\n",
        "        target_encoder = ce.TargetEncoder(cols=categorical_columns,\n",
        "                                          handle_unknown='value',\n",
        "                                          handle_missing='value')\n",
        "        x = target_encoder.fit_transform(x, y)\n",
        "\n",
        "    return x, y, target_encoder\n",
        "\n",
        "cb_X_train_raw, cb_y_train_raw, cat = cb_preprocessing(X_train_raw, y_train_raw)\n",
        "if DO_VALID:\n",
        "    cb_X_valid_raw, cb_y_valid_raw, _ = cb_preprocessing(X_valid_raw, y_valid_raw)\n",
        "\n",
        "rf_X_train_raw, rf_y_train_raw, te = rf_preprocessing(X_train_raw, y_train_raw, is_test=False)\n",
        "if DO_VALID:\n",
        "    rf_X_valid_raw, rf_y_valid_raw, _ = rf_preprocessing(X_valid_raw, y_valid_raw, is_test=True, TargetEncoder=te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIeMiDCWu_Pa",
        "outputId": "add4f4a9-cfb0-40f3-c41c-9dd74f5e1726"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Equipment_Dam</th>\n",
              "      <th>Model_Suffix_Dam</th>\n",
              "      <th>Workorder_Dam</th>\n",
              "      <th>CURE END POSITION X Collect Result_Dam</th>\n",
              "      <th>CURE END POSITION Z Collect Result_Dam</th>\n",
              "      <th>CURE END POSITION Θ Collect Result_Dam</th>\n",
              "      <th>CURE SPEED Collect Result_Dam</th>\n",
              "      <th>CURE START POSITION X Collect Result_Dam</th>\n",
              "      <th>CURE START POSITION Θ Collect Result_Dam</th>\n",
              "      <th>DISCHARGED SPEED OF RESIN Collect Result_Dam</th>\n",
              "      <th>...</th>\n",
              "      <th>Head Clean Position Z Collect Result_Fill2</th>\n",
              "      <th>Head Purge Position X Collect Result_Fill2</th>\n",
              "      <th>Head Purge Position Y Collect Result_Fill2</th>\n",
              "      <th>Head Purge Position Z Collect Result_Fill2</th>\n",
              "      <th>Machine Tact time Collect Result_Fill2</th>\n",
              "      <th>PalletID Collect Result_Fill2</th>\n",
              "      <th>Production Qty Collect Result_Fill2</th>\n",
              "      <th>Receip No Collect Result_Fill2</th>\n",
              "      <th>WorkMode Collect Result_Fill2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dam dispenser #2</td>\n",
              "      <td>AJX75334501</td>\n",
              "      <td>4C1XH186-2</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>12.5</td>\n",
              "      <td>90</td>\n",
              "      <td>85</td>\n",
              "      <td>280</td>\n",
              "      <td>90</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>50.0</td>\n",
              "      <td>91.8</td>\n",
              "      <td>270</td>\n",
              "      <td>50</td>\n",
              "      <td>114.612</td>\n",
              "      <td>20.0</td>\n",
              "      <td>11</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dam dispenser #2</td>\n",
              "      <td>AJX75334501</td>\n",
              "      <td>3H1X7976-1</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>12.5</td>\n",
              "      <td>90</td>\n",
              "      <td>70</td>\n",
              "      <td>280</td>\n",
              "      <td>90</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>91.8</td>\n",
              "      <td>270.0</td>\n",
              "      <td>50</td>\n",
              "      <td>85</td>\n",
              "      <td>19.900</td>\n",
              "      <td>9.0</td>\n",
              "      <td>173</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dam dispenser #1</td>\n",
              "      <td>AJX75334501</td>\n",
              "      <td>3G1X8303-1</td>\n",
              "      <td>240.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>-90</td>\n",
              "      <td>70</td>\n",
              "      <td>1030</td>\n",
              "      <td>-90</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>91.8</td>\n",
              "      <td>270.0</td>\n",
              "      <td>50</td>\n",
              "      <td>85</td>\n",
              "      <td>19.100</td>\n",
              "      <td>7.0</td>\n",
              "      <td>176</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dam dispenser #1</td>\n",
              "      <td>AJX75334501</td>\n",
              "      <td>3F1X9648-1</td>\n",
              "      <td>240.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>-90</td>\n",
              "      <td>70</td>\n",
              "      <td>1030</td>\n",
              "      <td>-90</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>91.8</td>\n",
              "      <td>270.0</td>\n",
              "      <td>50</td>\n",
              "      <td>85</td>\n",
              "      <td>18.900</td>\n",
              "      <td>5.0</td>\n",
              "      <td>68</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dam dispenser #2</td>\n",
              "      <td>AJX75334501</td>\n",
              "      <td>3M1XC491-1</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>12.5</td>\n",
              "      <td>90</td>\n",
              "      <td>70</td>\n",
              "      <td>280</td>\n",
              "      <td>90</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>50.0</td>\n",
              "      <td>91.8</td>\n",
              "      <td>270</td>\n",
              "      <td>50</td>\n",
              "      <td>85.000</td>\n",
              "      <td>19.7</td>\n",
              "      <td>14</td>\n",
              "      <td>424</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 146 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Equipment_Dam Model_Suffix_Dam Workorder_Dam  \\\n",
              "0  Dam dispenser #2      AJX75334501    4C1XH186-2   \n",
              "1  Dam dispenser #2      AJX75334501    3H1X7976-1   \n",
              "2  Dam dispenser #1      AJX75334501    3G1X8303-1   \n",
              "3  Dam dispenser #1      AJX75334501    3F1X9648-1   \n",
              "4  Dam dispenser #2      AJX75334501    3M1XC491-1   \n",
              "\n",
              "   CURE END POSITION X Collect Result_Dam  \\\n",
              "0                                  1000.0   \n",
              "1                                  1000.0   \n",
              "2                                   240.0   \n",
              "3                                   240.0   \n",
              "4                                  1000.0   \n",
              "\n",
              "   CURE END POSITION Z Collect Result_Dam  \\\n",
              "0                                    12.5   \n",
              "1                                    12.5   \n",
              "2                                     2.5   \n",
              "3                                     2.5   \n",
              "4                                    12.5   \n",
              "\n",
              "   CURE END POSITION Θ Collect Result_Dam  CURE SPEED Collect Result_Dam  \\\n",
              "0                                      90                             85   \n",
              "1                                      90                             70   \n",
              "2                                     -90                             70   \n",
              "3                                     -90                             70   \n",
              "4                                      90                             70   \n",
              "\n",
              "   CURE START POSITION X Collect Result_Dam  \\\n",
              "0                                       280   \n",
              "1                                       280   \n",
              "2                                      1030   \n",
              "3                                      1030   \n",
              "4                                       280   \n",
              "\n",
              "   CURE START POSITION Θ Collect Result_Dam  \\\n",
              "0                                        90   \n",
              "1                                        90   \n",
              "2                                       -90   \n",
              "3                                       -90   \n",
              "4                                        90   \n",
              "\n",
              "   DISCHARGED SPEED OF RESIN Collect Result_Dam  ...  \\\n",
              "0                                            16  ...   \n",
              "1                                            10  ...   \n",
              "2                                            10  ...   \n",
              "3                                            10  ...   \n",
              "4                                            16  ...   \n",
              "\n",
              "   Head Clean Position Z Collect Result_Fill2  \\\n",
              "0                                        50.0   \n",
              "1                                        91.8   \n",
              "2                                        91.8   \n",
              "3                                        91.8   \n",
              "4                                        50.0   \n",
              "\n",
              "   Head Purge Position X Collect Result_Fill2  \\\n",
              "0                                        91.8   \n",
              "1                                       270.0   \n",
              "2                                       270.0   \n",
              "3                                       270.0   \n",
              "4                                        91.8   \n",
              "\n",
              "   Head Purge Position Y Collect Result_Fill2  \\\n",
              "0                                         270   \n",
              "1                                          50   \n",
              "2                                          50   \n",
              "3                                          50   \n",
              "4                                         270   \n",
              "\n",
              "   Head Purge Position Z Collect Result_Fill2  \\\n",
              "0                                          50   \n",
              "1                                          85   \n",
              "2                                          85   \n",
              "3                                          85   \n",
              "4                                          50   \n",
              "\n",
              "   Machine Tact time Collect Result_Fill2  PalletID Collect Result_Fill2  \\\n",
              "0                                 114.612                           20.0   \n",
              "1                                  19.900                            9.0   \n",
              "2                                  19.100                            7.0   \n",
              "3                                  18.900                            5.0   \n",
              "4                                  85.000                           19.7   \n",
              "\n",
              "   Production Qty Collect Result_Fill2  Receip No Collect Result_Fill2  \\\n",
              "0                                   11                             125   \n",
              "1                                  173                               1   \n",
              "2                                  176                               1   \n",
              "3                                   68                               1   \n",
              "4                                   14                             424   \n",
              "\n",
              "   WorkMode Collect Result_Fill2  target  \n",
              "0                              1       0  \n",
              "1                              0       0  \n",
              "2                              0       0  \n",
              "3                              0       0  \n",
              "4                              1       0  \n",
              "\n",
              "[5 rows x 146 columns]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.concat([cb_X_train_raw,cb_y_train_raw], axis=1).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XIJg9dru_Pb"
      },
      "source": [
        "## 2.4 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5KGR5Tmu_Pb"
      },
      "outputs": [],
      "source": [
        "cat_features = cat.tolist()\n",
        "dam_columns = [col for col in cb_X_train_raw.columns if col.split('_')[-1] in [\"Dam\"]]\n",
        "fill1_columns = [col for col in cb_X_train_raw.columns if col.split('_')[-1] in [\"Fill1\"]]\n",
        "fill2_columns = [col for col in cb_X_train_raw.columns if col.split('_')[-1] in [\"Fill2\"]]\n",
        "autoclave_columns = [col for col in cb_X_train_raw.columns if col.split('_')[-1] in [\"AutoClave\"]]\n",
        "\n",
        "params = {\n",
        "#     'iterations': 3000,                # 최대 3000번의 반복\n",
        "    'learning_rate': 0.05,             # 학습률: 0.05 (초기값으로 적절히 설정)\n",
        "    'depth': 6,                        # 트리의 깊이: 6 (복잡한 모델을 방지하기 위해 중간값 설정)\n",
        "    'l2_leaf_reg': 3,                  # L2 정규화: 3 (모델 복잡도 제어)\n",
        "    'one_hot_max_size': 10,            # one-hot 인코딩으로 변환할 카테고리형 변수의 최대 크기\n",
        "    'random_seed': RANDOM_SEED,        # 재현성을 위한 랜덤 시드 설정\n",
        "    'task_type': \"CPU\",                # CPU 사용 (GPU로 변경 가능)\n",
        "    'loss_function': 'Logloss',        # 이진 분류를 위한 로그 손실 함수\n",
        "    'eval_metric': \"F1\",               # 평가 지표: F1 스코어\n",
        "    'auto_class_weights': 'Balanced',  # 자동 클래스 가중치: 불균형 데이터에 대응\n",
        "    'early_stopping_rounds': 500,      # 조기 종료를 위한 patience 설정\n",
        "    'verbose': 100                     # 100회 반복마다 결과 출력\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6P15tiRu_Pb"
      },
      "source": [
        "## 2.5 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c46F6ZtZu_Pb",
        "outputId": "e7eb494c-4fde-41b3-cdec-2415de6dee72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1978/2203771385.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  Dam_model1.fit(X_Dam, rf_y_train_raw)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6156927\ttest: 0.5680721\tbest: 0.5680721 (0)\ttotal: 33.8ms\tremaining: 7.23s\n",
            "100:\tlearn: 0.6264656\ttest: 0.5674946\tbest: 0.5831942 (20)\ttotal: 1.89s\tremaining: 2.13s\n",
            "200:\tlearn: 0.6484820\ttest: 0.5800907\tbest: 0.5831942 (20)\ttotal: 3.73s\tremaining: 260ms\n",
            "214:\tlearn: 0.6494000\ttest: 0.5865606\tbest: 0.5865606 (214)\ttotal: 3.94s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.5865605756\n",
            "bestIteration = 214\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1978/2203771385.py:30: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  Fill1_model1.fit(X_Fill1, rf_y_train_raw)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6070551\ttest: 0.5752161\tbest: 0.5752161 (0)\ttotal: 27.8ms\tremaining: 4.44s\n",
            "100:\tlearn: 0.6266228\ttest: 0.5821430\tbest: 0.5851321 (15)\ttotal: 1.2s\tremaining: 711ms\n",
            "160:\tlearn: 0.6405573\ttest: 0.5811095\tbest: 0.5851321 (15)\ttotal: 1.86s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.5851321095\n",
            "bestIteration = 15\n",
            "\n",
            "Shrink model to first 16 iterations.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1978/2203771385.py:39: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  Fill2_model1.fit(X_Fill2, rf_y_train_raw)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.5508904\ttest: 0.5253529\tbest: 0.5253529 (0)\ttotal: 24.8ms\tremaining: 322ms\n",
            "13:\tlearn: 0.6073907\ttest: 0.5716709\tbest: 0.5807360 (6)\ttotal: 207ms\tremaining: 0us\n",
            "\n",
            "bestTest = 0.5807359968\n",
            "bestIteration = 6\n",
            "\n",
            "Shrink model to first 7 iterations.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1978/2203771385.py:48: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  Auto_model.fit(X_Auto, rf_y_train_raw)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_Dam = rf_X_train_raw[dam_columns]\n",
        "X_Fill1 = rf_X_train_raw[fill1_columns]\n",
        "X_Fill2 = rf_X_train_raw[fill2_columns]\n",
        "X_Auto = rf_X_train_raw[autoclave_columns]\n",
        "_X_Dam = cb_X_train_raw[dam_columns]\n",
        "_X_Fill1 = cb_X_train_raw[fill1_columns]\n",
        "_X_Fill2 = cb_X_train_raw[fill2_columns]\n",
        "_X_Auto = cb_X_train_raw[autoclave_columns]\n",
        "\n",
        "if DO_VALID:\n",
        "    X_val_Dam = rf_X_valid_raw[dam_columns]\n",
        "    X_val_Fill1 = rf_X_valid_raw[fill1_columns]\n",
        "    X_val_Fill2 = rf_X_valid_raw[fill2_columns]\n",
        "    X_val_Auto = rf_X_valid_raw[autoclave_columns]\n",
        "    _X_val_Dam = cb_X_valid_raw[dam_columns]\n",
        "    _X_val_Fill1 = cb_X_valid_raw[fill1_columns]\n",
        "    _X_val_Fill2 = cb_X_valid_raw[fill2_columns]\n",
        "    _X_val_Auto = cb_X_valid_raw[autoclave_columns]\n",
        "\n",
        "Dam_model1 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
        "Dam_model1.fit(X_Dam, rf_y_train_raw)\n",
        "Dam_model2 = CatBoostClassifier(**params, cat_features= list(set(cat_features) & set(dam_columns)), iterations=215)\n",
        "\n",
        "if DO_VALID:\n",
        "    Dam_model2.fit(_X_Dam, cb_y_train_raw, eval_set = [(_X_val_Dam, cb_y_valid_raw)])\n",
        "else:\n",
        "    Dam_model2.fit(_X_Dam, cb_y_train_raw)\n",
        "\n",
        "Fill1_model1 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
        "Fill1_model1.fit(X_Fill1, rf_y_train_raw)\n",
        "Fill1_model2 = CatBoostClassifier(**params, cat_features= list(set(cat_features) & set(fill1_columns)), iterations=161)\n",
        "\n",
        "if DO_VALID:\n",
        "    Fill1_model2.fit(_X_Fill1, cb_y_train_raw, eval_set = [(_X_val_Fill1, cb_y_valid_raw)])\n",
        "else:\n",
        "    Fill1_model2.fit(_X_Fill1, cb_y_train_raw)\n",
        "\n",
        "Fill2_model1 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
        "Fill2_model1.fit(X_Fill2, rf_y_train_raw)\n",
        "Fill2_model2 = CatBoostClassifier(**params, cat_features= list(set(cat_features) & set(fill2_columns)), iterations=14)\n",
        "\n",
        "if DO_VALID:\n",
        "    Fill2_model2.fit(_X_Fill2, cb_y_train_raw, eval_set = [(_X_val_Fill2, cb_y_valid_raw)])\n",
        "else:\n",
        "    Fill2_model2.fit(_X_Fill2, cb_y_train_raw)\n",
        "\n",
        "Auto_model = RandomForestClassifier(random_state=RANDOM_SEED)\n",
        "Auto_model.fit(X_Auto, rf_y_train_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ApwM_I0u_Pc",
        "outputId": "3af6be94-a33e-46b1-f6df-e3073599997d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.0275, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.0275, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "IsolationForest(contamination=0.0275, random_state=42)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dam_proba1 = Dam_model1.predict_proba(X_Dam)\n",
        "dam_proba2 = Dam_model2.predict_proba(_X_Dam)\n",
        "fill1_proba1 = Fill1_model1.predict_proba(X_Fill1)\n",
        "fill1_proba2 = Fill1_model2.predict_proba(_X_Fill1)\n",
        "fill2_proba1 = Fill2_model1.predict_proba(X_Fill2)\n",
        "fill2_proba2 = Fill2_model2.predict_proba(_X_Fill2)\n",
        "auto_proba = Auto_model.predict_proba(X_Auto)\n",
        "\n",
        "probability = pd.DataFrame(np.column_stack((dam_proba1, fill1_proba1, fill2_proba1, auto_proba,\n",
        "                                           dam_proba2, fill1_proba2, fill2_proba2)))\n",
        "\n",
        "# Isolation Forest 모델 생성\n",
        "Main_Model = IsolationForest(contamination=0.0275, random_state=RANDOM_SEED)\n",
        "# 모델 학습\n",
        "Main_Model.fit(probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmNUOsxku_Pd"
      },
      "source": [
        "## 2.6 Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_U8tBZuu_Pk",
        "outputId": "3f0501b8-a7ce-4497-c584-861b732c2bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.22553191489361704\n"
          ]
        }
      ],
      "source": [
        "if DO_VALID:\n",
        "    dam_proba1 = Dam_model1.predict_proba(X_val_Dam)\n",
        "    fill1_proba1 = Fill1_model1.predict_proba(X_val_Fill1)\n",
        "    fill2_proba1 = Fill2_model1.predict_proba(X_val_Fill2)\n",
        "\n",
        "    dam_proba2 = Dam_model2.predict_proba(_X_val_Dam)\n",
        "    fill1_proba2 = Fill1_model2.predict_proba(_X_val_Fill1)\n",
        "    fill2_proba2 = Fill2_model2.predict_proba(_X_val_Fill2)\n",
        "\n",
        "    auto_proba = Auto_model.predict_proba(X_val_Auto)\n",
        "\n",
        "    val_probability = pd.DataFrame(np.column_stack((dam_proba1, fill1_proba1, fill2_proba1, auto_proba,\n",
        "                                               dam_proba2, fill1_proba2, fill2_proba2)))\n",
        "\n",
        "    test_pred = Main_Model.predict(val_probability)\n",
        "    test_pred = pd.DataFrame(test_pred).replace({1:\"Normal\", -1:\"AbNormal\"})\n",
        "\n",
        "    y_valid_raw = y_valid_raw.replace({0:\"Normal\", 1:\"AbNormal\"})\n",
        "    f1 = f1_score(y_valid_raw, test_pred, pos_label = \"AbNormal\")\n",
        "    print(f1)\n",
        "\n",
        "    test_pred.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUFDRpVTu_Pk"
      },
      "source": [
        "## 2.7. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z02tf2V-u_Pk",
        "outputId": "c1d497a8-6dca-48ca-f7c1-fa9f52d2a0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17361, 464)\n",
            "(17361, 145)\n"
          ]
        }
      ],
      "source": [
        "test_path = \"data/test.csv\"\n",
        "\n",
        "X_test_raw, y_test_raw = read_csv(test_path)\n",
        "print(X_test_raw.shape)\n",
        "X_test_raw.columns = X_test_raw.columns.str.replace('.', '_')\n",
        "drop_columns = set(X_test_raw.columns) - set(X_train_raw.columns)\n",
        "X_test_raw = X_test_raw.drop(columns=drop_columns)\n",
        "\n",
        "# 1. .OK -> nan\n",
        "cols = [\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\"]\n",
        "\n",
        "for col in cols:\n",
        "    X_test_raw.loc[X_test_raw[col] == \"OK\", col] = np.nan\n",
        "    X_test_raw[col] = X_test_raw[col].astype(float)\n",
        "print(X_test_raw.shape)\n",
        "_ = fill_na_with_knn(X_test_raw, is_test=True, imputers=im)\n",
        "cb_X_test_raw, _, _ = cb_preprocessing(X_test_raw)\n",
        "rf_X_test_raw, _, _ = rf_preprocessing(X_test_raw, is_test=True, TargetEncoder=te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp5iUEFXu_Pl"
      },
      "outputs": [],
      "source": [
        "X_test_Dam = rf_X_test_raw[dam_columns]\n",
        "X_test_Fill1 = rf_X_test_raw[fill1_columns]\n",
        "X_test_Fill2 = rf_X_test_raw[fill2_columns]\n",
        "_X_test_Dam = cb_X_test_raw[dam_columns]\n",
        "_X_test_Fill1 = cb_X_test_raw[fill1_columns]\n",
        "_X_test_Fill2 = cb_X_test_raw[fill2_columns]\n",
        "X_test_Auto = rf_X_test_raw[autoclave_columns]\n",
        "\n",
        "dam_proba1 = Dam_model1.predict_proba(X_test_Dam)\n",
        "fill1_proba1 = Fill1_model1.predict_proba(X_test_Fill1)\n",
        "fill2_proba1 = Fill2_model1.predict_proba(X_test_Fill2)\n",
        "\n",
        "dam_proba2 = Dam_model2.predict_proba(_X_test_Dam)\n",
        "fill1_proba2 = Fill1_model2.predict_proba(_X_test_Fill1)\n",
        "fill2_proba2 = Fill2_model2.predict_proba(_X_test_Fill2)\n",
        "\n",
        "auto_proba = Auto_model.predict_proba(X_test_Auto)\n",
        "\n",
        "test_probability = pd.DataFrame(np.column_stack((dam_proba1, fill1_proba1, fill2_proba1, auto_proba,\n",
        "                                           dam_proba2, fill1_proba2, fill2_proba2)))\n",
        "\n",
        "test_pred = Main_Model.predict(test_probability)\n",
        "test_pred = pd.DataFrame(test_pred).replace({1:\"Normal\", -1:\"AbNormal\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGIvMK0Nu_Pl",
        "outputId": "d771bbaa-8ea4-4cd8-cf99-0a4c48215428"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Normal      17044\n",
              "AbNormal      317\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_pred.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uKtLJ60u_Pl"
      },
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission_2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMiNIRV7u_Pl"
      },
      "source": [
        "# Model3: Random Forest and Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh0QmmaNu_Pl"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = \"/data\"\n",
        "RANDOM_STATE = 110\n",
        "DO_VALID = False\n",
        "train_data = pd.read_csv(TRAIN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9skKQRau_Pm"
      },
      "source": [
        "## 3.1 Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNx44ABau_Pm"
      },
      "outputs": [],
      "source": [
        "# target label 분리\n",
        "y_train = train_data['target']\n",
        "X_train = train_data.iloc[:,:-1]\n",
        "\n",
        "# Equipment 조합 Column 추가\n",
        "\n",
        "def feature_extraction(df):\n",
        "    def check_condition(row):\n",
        "        # 첫 번째 조건 확인\n",
        "        condition1 = (row['Equipment_Dam'] == 'Dam dispenser #1') and \\\n",
        "                     (row['Equipment_Fill1'] == 'Fill1 dispenser #1') and \\\n",
        "                     (row['Equipment_Fill2'] == 'Fill2 dispenser #1')\n",
        "\n",
        "        # 두 번째 조건 확인\n",
        "        condition2 = (row['Equipment_Dam'] == 'Dam dispenser #2') and \\\n",
        "                     (row['Equipment_Fill1'] == 'Fill1 dispenser #2') and \\\n",
        "                     (row['Equipment_Fill2'] == 'Fill2 dispenser #2')\n",
        "\n",
        "        # 두 조건 중 하나라도 만족하면 True 반환\n",
        "        return condition1 or condition2\n",
        "\n",
        "    # 새로운 컬럼 생성\n",
        "    df['Equipment_com'] = df.apply(check_condition, axis=1)\n",
        "    return df\n",
        "\n",
        "X_train = feature_extraction(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcm3rNUku_Pm"
      },
      "outputs": [],
      "source": [
        "# 결측치 평균값으로 보정\n",
        "\n",
        "def fill_na_with_mean(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():  # 결측치가 있는지 확인\n",
        "            mean_value = df[column].mean()  # 평균값 계산\n",
        "            df[column].fillna(mean_value, inplace=True)\n",
        "# 결측치를 최빈값으로 대체\n",
        "fill_na_with_mean(X_train)\n",
        "\n",
        "#Target Encoding 하기 위해 AbNormal과 Normal을 숫자값으로 변환\n",
        "\n",
        "y_train_encoded = y_train.replace({'AbNormal': 1, 'Normal': 0})\n",
        "object_columns = X_train.select_dtypes(include=['object'])\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "target_encoder = ce.TargetEncoder(cols=object_columns.columns, handle_unknown='value', handle_missing='value')\n",
        "X_train = target_encoder.fit_transform(X_train, y_train_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7u46sx9u_Pn"
      },
      "source": [
        "## 3.2 Train Set을 각 공정으로 Split 한 후 모델 학습 (RF -> IF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNuS6zxnu_Pn",
        "outputId": "381d3914-cf82-49b4-ff10-6f460821952a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.0275, random_state=110)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.0275, random_state=110)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "IsolationForest(contamination=0.0275, random_state=110)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_Dam = X_train.filter(like='Dam')\n",
        "X_Fill1 = X_train.filter(like='Fill1')\n",
        "X_Fill2 = X_train.filter(like=\"Fill2\")\n",
        "X_Auto = X_train.filter(like='Auto')\n",
        "\n",
        "Dam_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Dam_model.fit(X_Dam, y_train)\n",
        "\n",
        "Fill1_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Fill1_model.fit(X_Fill1, y_train)\n",
        "\n",
        "Fill2_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Fill2_model.fit(X_Fill2, y_train)\n",
        "\n",
        "Auto_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Auto_model.fit(X_Auto, y_train)\n",
        "\n",
        "\n",
        "dam_proba = Dam_model.predict_proba(X_Dam)\n",
        "fill1_proba = Fill1_model.predict_proba(X_Fill1)\n",
        "fill2_proba = Fill2_model.predict_proba(X_Fill2)\n",
        "auto_proba = Auto_model.predict_proba(X_Auto)\n",
        "probability = pd.DataFrame(np.column_stack((dam_proba, fill1_proba, fill2_proba, auto_proba)))\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "# Isolation Forest 모델 생성\n",
        "Main_Model = IsolationForest(contamination=0.0275, random_state=RANDOM_STATE)\n",
        "# 모델 학습\n",
        "Main_Model.fit(probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG2HNnJku_Pn"
      },
      "source": [
        "## 3.3 Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te1BMwqku_Pn"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    test_data = pd.read_csv(VALID_PATH)\n",
        "\n",
        "    #target label 분리\n",
        "    y_val = test_data['target']\n",
        "    X_val = test_data.iloc[:,:-1]\n",
        "\n",
        "    # preprocessing\n",
        "    y_val_encoded = y_val.replace({'AbNormal': 1, 'Normal': 0})\n",
        "    fill_na_with_mean(X_val)\n",
        "    X_val = feature_extraction(X_val)\n",
        "    X_val = target_encoder.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2y11YlWu_Po"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    # 만들어 놓은 모델로 확률값 계산\n",
        "    X_val_Dam = X_val.filter(like='Dam')\n",
        "    X_val_Fill1 = X_val.filter(like='Fill1')\n",
        "    X_val_Fill2 = X_val.filter(like=\"Fill2\")\n",
        "    X_val_Auto = X_val.filter(like='Auto')\n",
        "\n",
        "    dam_proba = Dam_model.predict_proba(X_val_Dam)\n",
        "    fill1_proba = Fill1_model.predict_proba(X_val_Fill1)\n",
        "    fill2_proba = Fill2_model.predict_proba(X_val_Fill2)\n",
        "    auto_proba = Auto_model.predict_proba(X_val_Auto)\n",
        "    val_probability = pd.DataFrame(np.hstack((dam_proba, fill1_proba, fill2_proba, auto_proba)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs5YmSmvu_Po"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    test_pred = Main_Model.predict(val_probability)\n",
        "    test_pred = pd.DataFrame(test_pred).replace({1:\"Normal\", -1 : \"AbNormal\"})\n",
        "\n",
        "    # Eqipment 조합에 따라 AbNormal 후처리 해주기\n",
        "    test_pred[X_val['Equipment_com'] == False] = 'AbNormal'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIgMXBUju_Po"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    from sklearn.metrics import f1_score\n",
        "\n",
        "    # F1 Score\n",
        "\n",
        "    f1 = f1_score(y_val, test_pred, pos_label = \"AbNormal\")\n",
        "    print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwS5xxbbu_Po"
      },
      "source": [
        "## 3.4 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTjTQp3wu_Po"
      },
      "outputs": [],
      "source": [
        "table = pd.read_csv(\"data/test.csv\")\n",
        "table.columns = table.columns.str.replace('.', '_')\n",
        "\n",
        "####### 전처리 by 의진  ################\n",
        "\n",
        "# 1. .OK -> nan\n",
        "cols = [\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\"]\n",
        "\n",
        "for col in cols:\n",
        "    table.loc[table[col] == \"OK\", col] = np.nan\n",
        "    table[col] = table[col].astype(float)\n",
        "\n",
        "# 2. 필요없는 Column버리기\n",
        "def find_unique_columns(df):\n",
        "    unique_domain_columns = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        unique_values = df[column].dropna().unique()\n",
        "        if len(unique_values) <= 1:\n",
        "            unique_domain_columns.append(column)\n",
        "\n",
        "    return unique_domain_columns\n",
        "\n",
        "unique_columns = find_unique_columns(table)\n",
        "removed_table = table.drop(columns=unique_columns)\n",
        "\n",
        "####################################\n",
        "\n",
        "\n",
        "#preprocess\n",
        "\n",
        "X_val = removed_table.iloc[:,1:]\n",
        "X_val = feature_extraction(X_val)\n",
        "fill_na_with_mean(X_val)\n",
        "X_val = target_encoder.transform(X_val)\n",
        "\n",
        "# filtering\n",
        "\n",
        "X_val_Dam = X_val.filter(like='Dam')\n",
        "X_val_Fill1 = X_val.filter(like='Fill1')\n",
        "X_val_Fill2 = X_val.filter(like=\"Fill2\")\n",
        "X_val_Auto = X_val.filter(like='Auto')\n",
        "\n",
        "#predict\n",
        "\n",
        "dam_proba = Dam_model.predict_proba(X_val_Dam)\n",
        "fill1_proba = Fill1_model.predict_proba(X_val_Fill1)\n",
        "fill2_proba = Fill2_model.predict_proba(X_val_Fill2)\n",
        "auto_proba = Auto_model.predict_proba(X_val_Auto)\n",
        "val_probability = pd.DataFrame(np.hstack((dam_proba, fill1_proba, fill2_proba, auto_proba)))\n",
        "\n",
        "test_pred = Main_Model.predict(val_probability)\n",
        "test_pred = pd.DataFrame(test_pred).replace({1:\"Normal\", -1 : \"AbNormal\"})\n",
        "test_pred[X_val['Equipment_com'] == False] = 'AbNormal'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmvOFoIDu_Pp"
      },
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission_3.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWG4zNgEu_Pp"
      },
      "source": [
        "# Model4: RF and AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8LPf3epu_Pp"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 110\n",
        "train_data = pd.read_csv(TRAIN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eqSdoX9u_Pp"
      },
      "outputs": [],
      "source": [
        "# Target 열 뺴기\n",
        "y_train = train_data['target']\n",
        "X_train = train_data.iloc[:,:-1]\n",
        "\n",
        "#Equipment 조합에 따른 추가\n",
        "def feature_extraction(df):\n",
        "    def check_condition(row):\n",
        "        # 첫 번째 조건 확인\n",
        "        condition1 = (row['Equipment_Dam'] == 'Dam dispenser #1') and \\\n",
        "                     (row['Equipment_Fill1'] == 'Fill1 dispenser #1') and \\\n",
        "                     (row['Equipment_Fill2'] == 'Fill2 dispenser #1')\n",
        "\n",
        "        # 두 번째 조건 확인\n",
        "        condition2 = (row['Equipment_Dam'] == 'Dam dispenser #2') and \\\n",
        "                     (row['Equipment_Fill1'] == 'Fill1 dispenser #2') and \\\n",
        "                     (row['Equipment_Fill2'] == 'Fill2 dispenser #2')\n",
        "\n",
        "        # 두 조건 중 하나라도 만족하면 True 반환\n",
        "        return condition1 or condition2\n",
        "\n",
        "    # 새로운 컬럼 생성\n",
        "    df['Equipment_com'] = df.apply(check_condition, axis=1)\n",
        "    return df\n",
        "X_train = feature_extraction(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-C9O0znu_Pq"
      },
      "outputs": [],
      "source": [
        "#결측치 평균값 보정\n",
        "\n",
        "def fill_na_with_mean(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():  # 결측치가 있는지 확인\n",
        "            mean_value = df[column].mean()  # 평균값 계산\n",
        "            df[column].fillna(mean_value, inplace=True)\n",
        "fill_na_with_mean(X_train)\n",
        "\n",
        "# Target Encoder 쓰기 위해 label 변경\n",
        "y_train_encoded = y_train.replace({'AbNormal': 1, 'Normal': 0})\n",
        "object_columns = X_train.select_dtypes(include=['object'])\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "target_encoder = ce.TargetEncoder(cols=object_columns.columns, handle_unknown='value', handle_missing='value')\n",
        "X_train = target_encoder.fit_transform(X_train, y_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiDQ2igTu_Pq"
      },
      "outputs": [],
      "source": [
        "#Model 나누기\n",
        "\n",
        "X_Dam = X_train.filter(like='Dam')\n",
        "X_Fill1 = X_train.filter(like='Fill1')\n",
        "X_Fill2 = X_train.filter(like=\"Fill2\")\n",
        "X_Auto = X_train.filter(like='Auto')\n",
        "\n",
        "Dam_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Dam_model.fit(X_Dam, y_train)\n",
        "\n",
        "Fill1_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Fill1_model.fit(X_Fill1, y_train)\n",
        "\n",
        "Fill2_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Fill2_model.fit(X_Fill2, y_train)\n",
        "\n",
        "Auto_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Auto_model.fit(X_Auto, y_train)\n",
        "\n",
        "\n",
        "dam_proba = Dam_model.predict_proba(X_Dam)\n",
        "fill1_proba = Fill1_model.predict_proba(X_Fill1)\n",
        "fill2_proba = Fill2_model.predict_proba(X_Fill2)\n",
        "auto_proba = Auto_model.predict_proba(X_Auto)\n",
        "probability = pd.DataFrame(np.column_stack((dam_proba, fill1_proba, fill2_proba, auto_proba)))\n",
        "probability = StandardScaler().fit_transform(probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5DYGRhSu_Pq"
      },
      "outputs": [],
      "source": [
        "## Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VHgRO2Wu_Pq",
        "outputId": "890ab188-7ac1-4c57-d5ef-7773db526db7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-30 08:41:32.971404: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-08-30 08:41:33.010110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-30 08:41:36.185178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "def build_autoencoder(input_dim, encoding_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "def train_autoencoder(train_x, encoding_dim=14, epochs=50, batch_size=256):\n",
        "    input_dim = train_x.shape[1]\n",
        "    autoencoder = build_autoencoder(input_dim, encoding_dim)\n",
        "\n",
        "    autoencoder.fit(train_x, train_x,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
        "    return autoencoder, encoder\n",
        "\n",
        "def calculate_reconstruction_error(autoencoder, data):\n",
        "    reconstructed_data = autoencoder.predict(data)\n",
        "    reconstruction_error = np.mean(np.square(data - reconstructed_data), axis=1)\n",
        "    return reconstruction_error\n",
        "\n",
        "def classify_errors(errors, threshold):\n",
        "    return np.where(errors > threshold, \"AbNormal\", \"Normal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tbkhsq6u_Pq",
        "outputId": "2a758132-58b9-49da-db80-9cfd6e189489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "104/104 [==============================] - 1s 3ms/step - loss: 0.3703 - val_loss: 3.9582\n",
            "Epoch 2/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.2453 - val_loss: 3.8208\n",
            "Epoch 3/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1672 - val_loss: 3.7687\n",
            "Epoch 4/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1431 - val_loss: 3.7384\n",
            "Epoch 5/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1330 - val_loss: 3.7261\n",
            "Epoch 6/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1270 - val_loss: 3.7004\n",
            "Epoch 7/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1234 - val_loss: 3.6624\n",
            "Epoch 8/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1210 - val_loss: 3.5731\n",
            "Epoch 9/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1194 - val_loss: 3.4565\n",
            "Epoch 10/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1183 - val_loss: 3.3741\n",
            "Epoch 11/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1174 - val_loss: 3.3470\n",
            "Epoch 12/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1168 - val_loss: 3.3386\n",
            "Epoch 13/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1162 - val_loss: 3.3334\n",
            "Epoch 14/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1157 - val_loss: 3.3299\n",
            "Epoch 15/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1153 - val_loss: 3.3242\n",
            "Epoch 16/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1149 - val_loss: 3.3181\n",
            "Epoch 17/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 3.3141\n",
            "Epoch 18/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1142 - val_loss: 3.3085\n",
            "Epoch 19/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1138 - val_loss: 3.3036\n",
            "Epoch 20/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1135 - val_loss: 3.2984\n",
            "Epoch 21/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1132 - val_loss: 3.2926\n",
            "Epoch 22/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1130 - val_loss: 3.2860\n",
            "Epoch 23/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1128 - val_loss: 3.2797\n",
            "Epoch 24/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1126 - val_loss: 3.2736\n",
            "Epoch 25/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1124 - val_loss: 3.2687\n",
            "Epoch 26/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1123 - val_loss: 3.2659\n",
            "Epoch 27/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1121 - val_loss: 3.2642\n",
            "Epoch 28/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1120 - val_loss: 3.2619\n",
            "Epoch 29/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1119 - val_loss: 3.2598\n",
            "Epoch 30/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1119 - val_loss: 3.2597\n",
            "Epoch 31/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1118 - val_loss: 3.2579\n",
            "Epoch 32/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1117 - val_loss: 3.2573\n",
            "Epoch 33/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1117 - val_loss: 3.2563\n",
            "Epoch 34/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1116 - val_loss: 3.2563\n",
            "Epoch 35/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 3.2554\n",
            "Epoch 36/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 3.2549\n",
            "Epoch 37/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 3.2548\n",
            "Epoch 38/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 3.2545\n",
            "Epoch 39/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1114 - val_loss: 3.2543\n",
            "Epoch 40/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1114 - val_loss: 3.2541\n",
            "Epoch 41/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1114 - val_loss: 3.2545\n",
            "Epoch 42/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1113 - val_loss: 3.2538\n",
            "Epoch 43/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1113 - val_loss: 3.2534\n",
            "Epoch 44/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1113 - val_loss: 3.2537\n",
            "Epoch 45/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1112 - val_loss: 3.2537\n",
            "Epoch 46/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 3.2533\n",
            "Epoch 47/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 3.2538\n",
            "Epoch 48/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1112 - val_loss: 3.2539\n",
            "Epoch 49/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1111 - val_loss: 3.2533\n",
            "Epoch 50/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1111 - val_loss: 3.2533\n",
            "Epoch 51/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1111 - val_loss: 3.2531\n",
            "Epoch 52/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 3.2529\n",
            "Epoch 53/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 3.2532\n",
            "Epoch 54/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 3.2530\n",
            "Epoch 55/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1110 - val_loss: 3.2528\n",
            "Epoch 56/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1110 - val_loss: 3.2528\n",
            "Epoch 57/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 3.2530\n",
            "Epoch 58/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1110 - val_loss: 3.2526\n",
            "Epoch 59/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 3.2528\n",
            "Epoch 60/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1110 - val_loss: 3.2525\n",
            "Epoch 61/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 3.2525\n",
            "Epoch 62/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1109 - val_loss: 3.2524\n",
            "Epoch 63/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 3.2524\n",
            "Epoch 64/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 3.2523\n",
            "Epoch 65/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 3.2523\n",
            "Epoch 66/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1109 - val_loss: 3.2522\n",
            "Epoch 67/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1109 - val_loss: 3.2522\n",
            "Epoch 68/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1109 - val_loss: 3.2522\n",
            "Epoch 69/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1109 - val_loss: 3.2522\n",
            "Epoch 70/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 3.2522\n",
            "Epoch 71/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1108 - val_loss: 3.2522\n",
            "Epoch 72/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 73/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 74/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 75/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 76/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 77/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 78/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 79/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1108 - val_loss: 3.2521\n",
            "Epoch 80/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 81/100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 82/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 83/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 84/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 85/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 86/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 87/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 3.2520\n",
            "Epoch 88/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 89/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 90/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 91/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 92/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1107 - val_loss: 3.2521\n",
            "Epoch 93/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1106 - val_loss: 3.2521\n",
            "Epoch 94/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1106 - val_loss: 3.2521\n",
            "Epoch 95/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1106 - val_loss: 3.2521\n",
            "Epoch 96/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 3.2521\n",
            "Epoch 97/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 3.2521\n",
            "Epoch 98/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 3.2521\n",
            "Epoch 99/100\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 3.2521\n",
            "Epoch 100/100\n",
            "104/104 [==============================] - 0s 1ms/step - loss: 0.1106 - val_loss: 3.2521\n"
          ]
        }
      ],
      "source": [
        "autoencoder, encoder = train_autoencoder(probability, encoding_dim=14, epochs=100, batch_size=256)\n",
        "# autoencoder, encoder = train_autoencoder(probability, encoding_dim=32, epochs=75, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spDlcWPPu_Pr"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    test_data = pd.read_csv(VALID_PATH)\n",
        "    y_val = test_data['target']\n",
        "    X_val = test_data.iloc[:,:-1]\n",
        "    y_val_encoded = y_val.replace({'AbNormal': 1, 'Normal': 0})\n",
        "    fill_na_with_mean(X_val)\n",
        "    X_val = feature_extraction(X_val)\n",
        "    X_val = target_encoder.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqe4DxFVu_Pr"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    #공정에 따라 나누기\n",
        "\n",
        "    X_val_Dam = X_val.filter(like='Dam')\n",
        "    X_val_Fill1 = X_val.filter(like='Fill1')\n",
        "    X_val_Fill2 = X_val.filter(like=\"Fill2\")\n",
        "    X_val_Auto = X_val.filter(like='Auto')\n",
        "\n",
        "    dam_proba = Dam_model.predict_proba(X_val_Dam)\n",
        "    fill1_proba = Fill1_model.predict_proba(X_val_Fill1)\n",
        "    fill2_proba = Fill2_model.predict_proba(X_val_Fill2)\n",
        "    auto_proba = Auto_model.predict_proba(X_val_Auto)\n",
        "    val_probability = pd.DataFrame(np.hstack((dam_proba, fill1_proba, fill2_proba, auto_proba)))\n",
        "    val_probability = StandardScaler().fit_transform(val_probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERrChMwhu_Pr"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    # 재구성 오류 계산\n",
        "    test_reconstruction_error = calculate_reconstruction_error(autoencoder, val_probability)\n",
        "\n",
        "    # 오류에 기반한 이상치 판단 기준 설정 (예: 96% 이상의 재구성 오류를 이상치로 판단)\n",
        "\n",
        "    threshold = np.percentile(test_reconstruction_error, 96)\n",
        "    test_pred = classify_errors(test_reconstruction_error, threshold)\n",
        "    test_pred = pd.DataFrame(test_pred)\n",
        "    test_pred[X_val['Equipment_com'] == False] = 'AbNormal'\n",
        "\n",
        "    test_pred.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baYhHc1Pu_Pr"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "\n",
        "    # 데이터 스플릿으로 y_valid와 모델 예측으로 y_pred를 구한 후 실행\n",
        "    # 모델 검정이 없다면 y_true값으로 y_valid 대체\n",
        "    f1 = f1_score(y_val, test_pred, pos_label = \"AbNormal\")\n",
        "    print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Hl3bDzu_Pr"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFMa-zb3u_Ps",
        "outputId": "daa89a85-0a2e-4498-b295-ab16f0b5ffe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "543/543 [==============================] - 0s 658us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Normal      16488\n",
              "AbNormal      873\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table = pd.read_csv(TEST_PATH)\n",
        "table.columns = table.columns.str.replace('.', '_')\n",
        "\n",
        "# 1. .OK -> nan\n",
        "cols = [\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\"]\n",
        "\n",
        "for col in cols:\n",
        "    table.loc[table[col] == \"OK\", col] = np.nan\n",
        "    table[col] = table[col].astype(float)\n",
        "\n",
        "# 2. 필요없는 Column버리기\n",
        "def find_unique_columns(df):\n",
        "    unique_domain_columns = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        unique_values = df[column].dropna().unique()\n",
        "        if len(unique_values) <= 1:\n",
        "            unique_domain_columns.append(column)\n",
        "\n",
        "    return unique_domain_columns\n",
        "\n",
        "unique_columns = find_unique_columns(table)\n",
        "removed_table = table.drop(columns=unique_columns)\n",
        "\n",
        "X_val = removed_table.iloc[:,1:]\n",
        "X_val = feature_extraction(X_val)\n",
        "fill_na_with_mean(X_val)\n",
        "X_val = target_encoder.transform(X_val)\n",
        "\n",
        "X_val_Dam = X_val.filter(like='Dam')\n",
        "X_val_Fill1 = X_val.filter(like='Fill1')\n",
        "X_val_Fill2 = X_val.filter(like=\"Fill2\")\n",
        "X_val_Auto = X_val.filter(like='Auto')\n",
        "\n",
        "dam_proba = Dam_model.predict_proba(X_val_Dam)\n",
        "fill1_proba = Fill1_model.predict_proba(X_val_Fill1)\n",
        "fill2_proba = Fill2_model.predict_proba(X_val_Fill2)\n",
        "auto_proba = Auto_model.predict_proba(X_val_Auto)\n",
        "val_probability = pd.DataFrame(np.hstack((dam_proba, fill1_proba, fill2_proba, auto_proba)))\n",
        "val_probability = StandardScaler().fit_transform(val_probability)\n",
        "\n",
        "test_reconstruction_error = calculate_reconstruction_error(autoencoder, val_probability)\n",
        "\n",
        "# 오류에 기반한 이상치 판단 기준 설정 (예: 95% 이상의 재구성 오류를 이상치로 판단)\n",
        "threshold = np.percentile(test_reconstruction_error, 95)\n",
        "test_pred = classify_errors(test_reconstruction_error, threshold)\n",
        "test_pred = pd.DataFrame(test_pred)\n",
        "test_pred[X_val['Equipment_com'] == False] = 'AbNormal'\n",
        "test_pred.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGUeXM3ru_Ps"
      },
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission_4.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgFj99e8u_Ps"
      },
      "source": [
        "# Model5: One Class SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwqj8A-Tu_Ps"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 110\n",
        "\n",
        "train_data = pd.read_csv(TRAIN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dgIii63u_Ps"
      },
      "outputs": [],
      "source": [
        "y_train = train_data['target']\n",
        "X_train = train_data.iloc[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKDjj4_mu_Pt"
      },
      "outputs": [],
      "source": [
        "def fill_na_with_mean(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():  # 결측치가 있는지 확인\n",
        "            mean_value = df[column].mean()  # 평균값 계산\n",
        "            df[column].fillna(mean_value, inplace=True)\n",
        "# 결측치를 최빈값으로 대체\n",
        "fill_na_with_mean(X_train)\n",
        "y_train_encoded = y_train.replace({'AbNormal': 1, 'Normal': 0})\n",
        "object_columns = X_train.select_dtypes(include=['object'])\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "target_encoder = ce.TargetEncoder(cols=object_columns.columns, handle_unknown='value', handle_missing='value')\n",
        "X_train = target_encoder.fit_transform(X_train, y_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4DcP4Wuu_Pt"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "    test_data = pd.read_csv(VALID_PATH)\n",
        "    y_val = test_data['target']\n",
        "    X_val = test_data.iloc[:,:-1]\n",
        "    y_val_encoded = y_val.replace({'AbNormal': -1, 'Normal': 1})\n",
        "    fill_na_with_mean(X_val)\n",
        "    X_val = target_encoder.transform(X_val)\n",
        "    X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_VK8zRcu_Pt",
        "outputId": "cd7cd8eb-b91f-49e6-d27f-7fd68386b6ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneClassSVM(gamma=&#x27;auto&#x27;, nu=0.165)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneClassSVM</label><div class=\"sk-toggleable__content\"><pre>OneClassSVM(gamma=&#x27;auto&#x27;, nu=0.165)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "OneClassSVM(gamma='auto', nu=0.165)"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "X_Auto = X_train.filter(like='Auto')\n",
        "X_Result = X_train.filter(like='Result')\n",
        "#X_Dam = X_train.filter(like='Dam')\n",
        "X_Fill1 = X_train.filter(like='Fill1')\n",
        "#X_Fill2 = X_train.filter(like=\"Fill2\")\n",
        "#X_Stage = X_train.filter(like=\"Stage\")\n",
        "\n",
        "\n",
        "\n",
        "Auto_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Auto_model.fit(X_Auto, y_train)\n",
        "Result_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Result_model.fit(X_Result, y_train)\n",
        "# Dam_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "# Dam_model.fit(X_Dam, y_train)\n",
        "Fill1_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "Fill1_model.fit(X_Fill1, y_train)\n",
        "# Fill2_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "# Fill2_model.fit(X_Fill2, y_train)\n",
        "# Stage_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "# Stage_model.fit(X_Stage, y_train)\n",
        "\n",
        "\n",
        "auto_proba = Auto_model.predict_proba(X_Auto)\n",
        "result_proba = Result_model.predict_proba(X_Result)\n",
        "# dam_proba = Dam_model.predict_proba(X_Dam)\n",
        "fill1_proba = Fill1_model.predict_proba(X_Fill1)\n",
        "#fill2_proba = Fill2_model.predict_proba(X_Fill2)\n",
        "#stage_proba = Stage_model.predict_proba(X_Stage)\n",
        "\n",
        "\n",
        "probability = pd.DataFrame(np.column_stack((auto_proba,result_proba,fill1_proba)))\n",
        "\n",
        "\n",
        "# One-Class SVM 모델 생성\n",
        "ocsvm_model = OneClassSVM(kernel='rbf', gamma=\"auto\", nu=0.165)  # gamma 0.01\n",
        "ocsvm_model.fit(probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b98MUa2Du_Pt"
      },
      "outputs": [],
      "source": [
        "X_val_Auto = X_val.filter(like='Auto')\n",
        "X_val_Result = X_val.filter(like='Result')\n",
        "#X_val_Dam = X_val.filter(like='Dam')\n",
        "X_val_Fill1 = X_val.filter(like='Fill1')\n",
        "#X_val_Fill2 = X_val.filter(like=\"Fill2\")\n",
        "#X_val_Stage = X_val.filter(like=\"Stage\")\n",
        "\n",
        "\n",
        "auto_proba = Auto_model.predict_proba(X_val_Auto)\n",
        "result_proba = Result_model.predict_proba(X_val_Result)\n",
        "#dam_proba = Dam_model.predict_proba(X_val_Dam)\n",
        "fill1_proba = Fill1_model.predict_proba(X_val_Fill1)\n",
        "#fill2_proba = Fill2_model.predict_proba(X_val_Fill2)\n",
        "#stage_proba = Speed_model.predict_proba(X_val_Speed)\n",
        "\n",
        "\n",
        "val_probability = pd.DataFrame(np.hstack((auto_proba,result_proba,fill1_proba)))#np.hstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iRb83JLu_Pt",
        "outputId": "03bddd33-03c6-4156-9065-144d41de9861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "# 예측 수행 (1: 정상, -1: 비정상)\n",
        "ocsvm_predictions = ocsvm_model.predict(val_probability)\n",
        "\n",
        "# 결과 확인 (정상: 1, 비정상: -1)\n",
        "print(ocsvm_predictions)\n",
        "scaler = StandardScaler()\n",
        "X_val_scaled = scaler.fit_transform(val_probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ6Bi0eiu_Pt"
      },
      "outputs": [],
      "source": [
        "ocsvm_predictions = pd.DataFrame(ocsvm_predictions).replace({1:\"Normal\", -1 : \"AbNormal\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcWYyOsDu_Pu",
        "outputId": "e090fc96-d979-4d9d-d9f0-cab478e4ad60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Normal      16757\n",
              "AbNormal      604\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ocsvm_predictions.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAy4ETa7u_Pu"
      },
      "outputs": [],
      "source": [
        "table = pd.read_csv(TEST_PATH)\n",
        "table.columns = table.columns.str.replace('.', '_')\n",
        "\n",
        "# 1. .OK -> nan\n",
        "cols = [\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\",\n",
        "        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\"]\n",
        "\n",
        "for col in cols:\n",
        "    table.loc[table[col] == \"OK\", col] = np.nan\n",
        "    table[col] = table[col].astype(float)\n",
        "\n",
        "# 2. 필요없는 Column버리기\n",
        "def find_unique_columns(df):\n",
        "    unique_domain_columns = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        unique_values = df[column].dropna().unique()\n",
        "        if len(unique_values) <= 1:\n",
        "            unique_domain_columns.append(column)\n",
        "\n",
        "    return unique_domain_columns\n",
        "\n",
        "unique_columns = find_unique_columns(table)\n",
        "removed_table = table.drop(columns=unique_columns)\n",
        "\n",
        "X_val = removed_table.iloc[:,1:]\n",
        "fill_na_with_mean(X_val)\n",
        "X_val = target_encoder.transform(X_val)\n",
        "\n",
        "X_val_Auto = X_val.filter(like='Auto')\n",
        "X_val_Result = X_val.filter(like='Result')\n",
        "X_val_Fill1 = X_val.filter(like='Fill1')\n",
        "\n",
        "auto_proba = Auto_model.predict_proba(X_val_Auto)\n",
        "result_proba = Result_model.predict_proba(X_val_Result)\n",
        "fill1_proba = Fill1_model.predict_proba(X_val_Fill1)\n",
        "val_probability = pd.DataFrame(np.hstack((auto_proba, result_proba, fill1_proba)))\n",
        "\n",
        "test_pred = ocsvm_model.predict(val_probability)\n",
        "test_pred = pd.DataFrame(test_pred).replace({1:\"Normal\", -1 : \"AbNormal\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO32wn-Su_Pu"
      },
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission_5.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC_OkecWu_Pu",
        "outputId": "8486763b-ddec-4c87-ec1e-c1b711e4cdec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "Normal      16757\n",
              "AbNormal      604\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sub['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SASspRK1u_Pu"
      },
      "source": [
        "# Hard Voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnLwaR0bu_Pv"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6kmZi8Dju_Pv"
      },
      "outputs": [],
      "source": [
        "\n",
        "DF1 = pd.read_csv(\"submission_1.csv\")\n",
        "DF2 = pd.read_csv(\"submission_2.csv\")\n",
        "DF3 = pd.read_csv(\"submission_3.csv\")\n",
        "DF4 = pd.read_csv(\"submission_4.csv\")\n",
        "DF5 = pd.read_csv(\"submission_5.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moHGD4TZu_Pv",
        "outputId": "bbd5f22b-b2ef-44fb-c991-62ebbad2b983"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>target</th>\n",
              "      <th>target</th>\n",
              "      <th>target</th>\n",
              "      <th>target</th>\n",
              "      <th>cc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17356</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17357</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17358</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17359</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17360</th>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17361 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       target  target  target  target  target      cc\n",
              "0      Normal  Normal  Normal  Normal  Normal  Normal\n",
              "1      Normal  Normal  Normal  Normal  Normal  Normal\n",
              "2      Normal  Normal  Normal  Normal  Normal  Normal\n",
              "3      Normal  Normal  Normal  Normal  Normal  Normal\n",
              "4      Normal  Normal  Normal  Normal  Normal  Normal\n",
              "...       ...     ...     ...     ...     ...     ...\n",
              "17356  Normal  Normal  Normal  Normal  Normal  Normal\n",
              "17357  Normal  Normal  Normal  Normal  Normal  Normal\n",
              "17358  Normal  Normal  Normal  Normal  Normal  Normal\n",
              "17359  Normal  Normal  Normal  Normal  Normal  Normal\n",
              "17360  Normal  Normal  Normal  Normal  Normal  Normal\n",
              "\n",
              "[17361 rows x 6 columns]"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = pd.concat([DF1['target'], DF2['target'], DF3['target'], DF4['target'], DF5['target']], axis =1)\n",
        "predictions = predictions.replace({\"Normal\" : 0, \"AbNormal\" : 1})\n",
        "predictions['cc'] = predictions.sum(axis=1).apply(lambda x: 1 if x >= 2 else 0)\n",
        "predictions = predictions.replace({ 0 : \"Normal\", 1 : \"AbNormal\"})\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIgZfpiXu_Pv"
      },
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = predictions['cc']\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28ynOCv_u_Pv",
        "outputId": "0bf07936-c520-437a-d9d0-a6c00e835ae4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "Normal      16562\n",
              "AbNormal      799\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sub.value_counts('target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDgvCwcBu_Pv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvHrcN3Iu_Pv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}